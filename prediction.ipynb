# Ignore warnings 
import warnings 
warnings.filterwarnings('ignore') 
# Handle table-like data and matrices 
import numpy as np 
import pandas as pd 
import math  
from sklearn.metrics import accuracy_score 
from sklearn.model_selection import train_test_split 
import keras 
# Visualisation 
import matplotlib.pyplot as plt 
import seaborn as sns 
# Configure visualisations 
%matplotlib inline 
sns.set_style( 'white' ) 
df = pd.read_csv('train.csv', encoding = 'utf-8') 
labels = pd.read_csv('labels.csv', encoding = 'utf-8') 
#One Hot encoding 
temp =[] 
for i in labels["In-hospital_death"]: 
 if i == 0: 
 temp.append([1,0]) 
 else: 
 temp.append([0,1]) 
temp = np.array(temp) 
new = pd.concat([df , labels] , axis = 1) 
print(new.shape) 
#Scaling Data 
from sklearn.preprocessing import MinMaxScaler 
scaler = MinMaxScaler(feature_range=(0, 1)) 
df = scaler.fit_transform(df) 
X = df 
y = temp
k = new["In-hospital_death"] 
X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2) a_train , a_test , b_train , b_test = train_test_split(X , k , test_size = 0.2) #importing Necessary Libraries 
from keras.models import Sequential 
from keras.layers import Dense, Dropout , BatchNormalization 
import pandas as pd 
from sklearn.model_selection import train_test_split 
from keras.optimizers import RMSprop, Adam 
from sklearn.metrics import confusion_matrix 
from sklearn.linear_model import LogisticRegression 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.ensemble import GradientBoostingClassifier 
from sklearn.naive_bayes import GaussianNB 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score, confusion_matrix,  classification_report 
import matplotlib.pyplot as plt 
from sklearn.metrics import roc_auc_score 
#Random Forest 
clf = RandomForestClassifier(random_state=42) 
clf.fit(X_train, y_train) # Reshape y_train using ravel() 
y_pred = clf.predict(X_test) 
accuracy = accuracy_score(y_test, y_pred) 
print("Accuracy:", accuracy*100) 
print("Classification Report:") 
print(classification_report(y_test, y_pred)) 
# Train a Gradient Boosting classifier 
clf = GradientBoostingClassifier(random_state=42) 
clf.fit(a_train, b_train) 
y_pred = clf.predict(a_test) 
accuracy = accuracy_score(b_test, y_pred) 
print("Accuracy:", accuracy*100) 
print("Confusion Matrix:") 
print(confusion_matrix(b_test, y_pred)) 
print("Classification Report:") 
print(classification_report(b_test, y_pred)) 
# Train a Gaussian Naive Bayes classifier 
clf = GaussianNB() 
clf.fit(a_train, b_train) 
y_pred = clf.predict(a_test) 
accuracy = accuracy_score(b_test, y_pred) 
print("Accuracy:", accuracy*100) 
print("Confusion Matrix:") 
print(confusion_matrix(b_test, y_pred)) 
print("Classification Report:") 
print(classification_report(b_test, y_pred)) 
# Train a Decision Tree classifier 
clf = DecisionTreeClassifier(random_state=42) 
clf.fit(a_train, b_train) 
y_pred = clf.predict(a_test) 
accuracy = accuracy_score(b_test, y_pred) 
print("Accuracy:", accuracy*100) 
print("Confusion Matrix:") 
print(confusion_matrix(b_test, y_pred))
print("Classification Report:") 
print(classification_report(b_test, y_pred)) 
#Building ANN 
model = Sequential() 
model.add(Dense(64, input_dim=X_train.shape[1] , activation='relu')) model.add(Dense(128, activation='relu')) 
model.add(Dense(196, activation='relu')) 
model.add(Dense(196, activation='relu')) 
model.add(BatchNormalization()) 
model.add(Dense(256, activation='relu')) 
model.add(Dense(2, activation='sigmoid')) 
model.compile(optimizer = Adam(lr = 0.0005),loss='binary_crossentropy',  metrics=['accuracy']) 
print(model.summary()) 
history = model.fit(X_train, y_train , epochs=15 , batch_size = 128 ,  validation_data=(X_test, y_test)) 
#visulaization 
plt.plot(history.history['accuracy']) 
plt.plot(history.history['val_accuracy']) 
plt.title('model accuracy') 
plt.ylabel('accuracy') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper left') 
plt.show() 
# summarize history for loss 
plt.plot(history.history['loss']) 
plt.plot(history.history['val_loss']) 
plt.title('model loss') 
plt.ylabel('loss') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper left') 
plt.show() 
#Accuracy visualization 
classifiers = [ 
 ('Random Forest', RandomForestClassifier(random_state=42)),  ('Gaussian Naive Bayes', GaussianNB()), 
 ('Decision Tree', DecisionTreeClassifier(random_state=42)),  ('Gradient Boosting classifier', GradientBoostingClassifier(random_state=42)) ] 
auc_roc_scores = [] 
for classifier_name, classifier in classifiers: 
 classifier.fit(a_train, b_train) 
 y_pred_proba = classifier.predict_proba(a_test)[:, 1] # Probability of  positive class 
 auc_roc = roc_auc_score(b_test, y_pred_proba) 
 auc_roc_scores.append((classifier_name, auc_roc)) 
auc_roc_scores = sorted(auc_roc_scores, key=lambda x: x[1], reverse=True) print("AUC-ROC Scores:") 
for classifier_name, auc_roc in auc_roc_scores: 
 print(f"{classifier_name}: {auc_roc*100}") 
print("Artifical Neural Network :" +str(acc*100))
